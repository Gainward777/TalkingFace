{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gainward777/TalkingFace/blob/main/TFaceUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IlXobLXTgl55"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "\n",
        "!pip install gradio\n",
        "\n",
        "\n",
        "!pip install elevenlabs -U\n",
        "\n",
        "\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "\n",
        "!pip3 install python-dotenv\n",
        "\n",
        "\n",
        "#!git clone https://github.com/yzhou359/MakeItTalk\n",
        "!git clone https://github.com/Gainward777/MakeItTalk\n",
        "%cd MakeItTalk/\n",
        "!export PYTHONPATH=/content/MakeItTalk:$PYTHONPATH\n",
        "!pip install -r requirements.txt\n",
        "!pip install tensorboardX\n",
        "\n",
        "!pip install Resemblyzer\n",
        "\n",
        "!pip install librosa==0.9.1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Libs and Downloads\n",
        "\n",
        "%cd MakeItTalk/\n",
        "!mkdir examples/dump\n",
        "!mkdir examples/ckpt\n",
        "!pip install gdown\n",
        "!gdown -O examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
        "!gdown -O examples/ckpt/ckpt_content_branch.pth https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
        "!gdown -O examples/ckpt/ckpt_speaker_branch.pth https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
        "!gdown -O examples/ckpt/ckpt_116_i2i_comb.pth https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
        "!gdown -O examples/dump/emb.pickle https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"thirdparty/AdaptiveWingLoss\")\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "from src.approaches.train_image_translation import Image_translation_block\n",
        "import torch\n",
        "import pickle\n",
        "import face_alignment\n",
        "from src.autovc.AutoVC_mel_Convertor_retrain_version import AutoVC_mel_Convertor\n",
        "import shutil\n",
        "import time\n",
        "import util.utils as util\n",
        "from scipy.signal import savgol_filter\n",
        "from src.approaches.train_audio2landmark import Audio2landmark_model\n",
        "\n",
        "from resemblyzer import preprocess_wav, VoiceEncoder\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resemblyzer_encoder = VoiceEncoder(device=device)\n",
        "\n",
        "predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, device='cpu', flip_input=True)\n",
        "\n",
        "\n",
        "\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "import requests\n",
        "\n",
        "import openai\n",
        "#openai.api_key=\n",
        "#OPENAI_API_KEY =\n",
        "\n",
        "\n",
        "\n",
        "from scipy.io.wavfile import write as write_wav #to temporarily save audio\n",
        "\n",
        "from elevenlabs import generate, play, set_api_key\n",
        "\n",
        "set_api_key('') #elevenlabs api key\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import shutil\n",
        "\n",
        "\n",
        "def get_spk_emb_prepr(audio_file_dir, resemblyzer_encoder, segment_len=960000):\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #resemblyzer_encoder = VoiceEncoder(device=device)\n",
        "\n",
        "    wav = preprocess_wav(audio_file_dir)\n",
        "    l = len(wav) // segment_len # segment_len = 16000 * 60\n",
        "    l = np.max([1, l])\n",
        "    all_embeds = []\n",
        "    for i in range(l):\n",
        "        mean_embeds, cont_embeds, wav_splits = resemblyzer_encoder.embed_utterance(\n",
        "            wav[segment_len * i:segment_len* (i + 1)], return_partials=True, rate=2)\n",
        "        all_embeds.append(mean_embeds)\n",
        "    all_embeds = np.array(all_embeds)\n",
        "    mean_embed = np.mean(all_embeds, axis=0)\n",
        "\n",
        "    return mean_embed, all_embeds\n",
        "\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "def get_response_form_ai(human_input, api_key, background, temperature, memory_len):\n",
        "    template=f\"{background}\" + \"\"\"\n",
        "    {history}\n",
        "    user: {human_input}\n",
        "    agent:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt=PromptTemplate(input_variables={\"history\", \"human_input\"}, template=template)\n",
        "\n",
        "    chatgpt_chain=LLMChain(llm=OpenAI(openai_api_key=api_key,temperature=temperature), prompt=prompt,\n",
        "                           verbose=True, memory=ConversationBufferWindowMemory(k=memory_len))\n",
        "\n",
        "    output=chatgpt_chain.predict(human_input=human_input)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "48d-jx7Ug96B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4d4d75-eb8c-467d-e32e-d739bd84538a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MakeItTalk\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
            "To: /content/MakeItTalk/examples/ckpt/ckpt_autovc.pth\n",
            "100% 172M/172M [00:02<00:00, 84.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
            "To: /content/MakeItTalk/examples/ckpt/ckpt_content_branch.pth\n",
            "100% 7.88M/7.88M [00:00<00:00, 50.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
            "To: /content/MakeItTalk/examples/ckpt/ckpt_speaker_branch.pth\n",
            "100% 15.4M/15.4M [00:00<00:00, 60.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
            "To: /content/MakeItTalk/examples/ckpt/ckpt_116_i2i_comb.pth\n",
            "100% 839M/839M [00:07<00:00, 116MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI\n",
            "To: /content/MakeItTalk/examples/dump/emb.pickle\n",
            "100% 30.9M/30.9M [00:00<00:00, 76.8MB/s]\n",
            "Loaded the voice encoder model on cuda in 8.84 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n",
            "100%|██████████| 85.7M/85.7M [00:05<00:00, 15.5MB/s]\n",
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/3DFAN4-4a694010b9.zip\" to /root/.cache/torch/hub/checkpoints/3DFAN4-4a694010b9.zip\n",
            "100%|██████████| 91.9M/91.9M [00:06<00:00, 13.8MB/s]\n",
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/depth-6c4283c0e0.zip\" to /root/.cache/torch/hub/checkpoints/depth-6c4283c0e0.zip\n",
            "100%|██████████| 224M/224M [00:20<00:00, 11.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Constants\n",
        "default_head_name=''\n",
        "AMP_LIP_SHAPE_X = 2 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
        "\n",
        "#@markdown Amplify the lip motion in vertical direction\n",
        "AMP_LIP_SHAPE_Y = 2 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
        "\n",
        "#@markdown Amplify the head pose motion (usually smaller than 1.0, put it to 0. for a static head pose)\n",
        "AMP_HEAD_POSE_MOTION = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "\n",
        "#@markdown Add naive eye blink\n",
        "ADD_NAIVE_EYE = True  #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "#@markdown If your image has an opened mouth, put this as True, else False\n",
        "CLOSE_INPUT_FACE_MOUTH = False  #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "\n",
        "#@markdown # Landmark Adjustment\n",
        "\n",
        "#@markdown Adjust upper lip thickness (postive value means thicker)\n",
        "UPPER_LIP_ADJUST = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:1.0}\n",
        "\n",
        "#@markdown Adjust lower lip thickness (postive value means thicker)\n",
        "LOWER_LIP_ADJUST = 1 #@param {type:\"slider\", min:-3.0, max:3.0, step:1.0}\n",
        "\n",
        "#@markdown Adjust static lip width (in multipication)\n",
        "LIP_WIDTH_ADJUST = 1.01 #@param {type:\"slider\", min:0.8, max:1.2, step:0.01}\n",
        "\n",
        "#@markdown Eye size (test func)\n",
        "LARGER_EYE=2 #@param {type:\"slider\", min:0, max:4, step:0.1}\n",
        "\n",
        "#@markdown # Voice Controllers\n",
        "#from google.colab import files\n",
        "'''\n",
        "print('Upload voice example. It must be \".wav ')\n",
        "voice_path = list(files.upload().keys())[0]\n",
        "'''\n",
        "\n",
        "TEMPERATURE=0.7 #@param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "\n",
        "BACKGROUND = \" you are as a role of Batman - Gotham Knight. Now lets playing the following requirements:  1) your name is Bruce Wayne, but no one knows that you are Batman;  2) you are 29 years old, you thunderstorm of crime, but sometimes you just want to be a gardener;  3) you have language addiction, sometimes you say [laughter] or [clears throat] at the and of the santance;  4) little by little you go crazy, sometimes you think you're really a bat;  5) Don't be overly enthusiastsic, but be cringe, don't be overly negative, don't be too boring.\" #@param {type: \"string\"}\n",
        "\n",
        "OPENAI_API_KEY = \"\" #@param {type: \"string\"}\n",
        "\n",
        "ELEVEN_LABS_API_KEY = \"\" #@param {type: \"string\"}\n",
        "\n",
        "MEMORY_LEN =2 #@param {type: \"integer\"}"
      ],
      "metadata": {
        "id": "6XzqvdV_iT9d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title _for test\n",
        "!pip install gradio\n",
        "import gradio as gr\n",
        "import shutil\n",
        "\n",
        "set_api_key(ELEVEN_LABS_API_KEY)"
      ],
      "metadata": {
        "id": "9JPavky1YofL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Funks\n",
        "def preprocess():\n",
        "    global opt_parser\n",
        "    global shape_3d\n",
        "    global scale\n",
        "    global shift\n",
        "    global AMP_LIP_SHAPE_X\n",
        "    global AMP_LIP_SHAPE_Y\n",
        "    global AMP_HEAD_POSE_MOTION\n",
        "    global ADD_NAIVE_EYE\n",
        "    global CLOSE_INPUT_FACE_MOUTH\n",
        "    global UPPER_LIP_ADJUST\n",
        "    global LOWER_LIP_ADJUST\n",
        "    global LIP_WIDTH_ADJUST\n",
        "    global LARGER_EYE\n",
        "    global img\n",
        "\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--jpg', type=str, default='{}.jpg'.format(default_head_name))\n",
        "    parser.add_argument('--close_input_face_mouth', default=CLOSE_INPUT_FACE_MOUTH, action='store_true')\n",
        "\n",
        "    parser.add_argument('--load_AUTOVC_name', type=str, default='examples/ckpt/ckpt_autovc.pth')\n",
        "    parser.add_argument('--load_a2l_G_name', type=str, default='examples/ckpt/ckpt_speaker_branch.pth')\n",
        "    parser.add_argument('--load_a2l_C_name', type=str, default='examples/ckpt/ckpt_content_branch.pth') #ckpt_audio2landmark_c.pth')\n",
        "    parser.add_argument('--load_G_name', type=str, default='examples/ckpt/ckpt_116_i2i_comb.pth') #ckpt_image2image.pth') #ckpt_i2i_finetune_150.pth') #c\n",
        "\n",
        "    parser.add_argument('--amp_lip_x', type=float, default=AMP_LIP_SHAPE_X)\n",
        "    parser.add_argument('--amp_lip_y', type=float, default=AMP_LIP_SHAPE_Y)\n",
        "    parser.add_argument('--amp_pos', type=float, default=AMP_HEAD_POSE_MOTION)\n",
        "    parser.add_argument('--reuse_train_emb_list', type=str, nargs='+', default=[]) #  ['iWeklsXc0H8']) #['45hn7-LXDX8']) #['E_kmpT-EfOg']) #'iWeklsXc0H8', '29k8RtSUjE0', '45hn7-LXDX8',\n",
        "    parser.add_argument('--add_audio_in', default=False, action='store_true')\n",
        "    parser.add_argument('--comb_fan_awing', default=False, action='store_true')\n",
        "    parser.add_argument('--output_folder', type=str, default='examples')\n",
        "\n",
        "    parser.add_argument('--test_end2end', default=True, action='store_true')\n",
        "    parser.add_argument('--dump_dir', type=str, default='', help='')\n",
        "    parser.add_argument('--pos_dim', default=7, type=int)\n",
        "    parser.add_argument('--use_prior_net', default=True, action='store_true')\n",
        "    parser.add_argument('--transformer_d_model', default=32, type=int)\n",
        "    parser.add_argument('--transformer_N', default=2, type=int)\n",
        "    parser.add_argument('--transformer_heads', default=2, type=int)\n",
        "    parser.add_argument('--spk_emb_enc_size', default=16, type=int)\n",
        "    parser.add_argument('--init_content_encoder', type=str, default='')\n",
        "    parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')\n",
        "    parser.add_argument('--reg_lr', type=float, default=1e-6, help='weight decay')\n",
        "    parser.add_argument('--write', default=False, action='store_true')\n",
        "    parser.add_argument('--segment_batch_size', type=int, default=1, help='batch size')\n",
        "    parser.add_argument('--emb_coef', default=3.0, type=float)\n",
        "    parser.add_argument('--lambda_laplacian_smooth_loss', default=1.0, type=float)\n",
        "    parser.add_argument('--use_11spk_only', default=False, action='store_true')\n",
        "    parser.add_argument('-f')\n",
        "\n",
        "    opt_parser = parser.parse_args()\n",
        "\n",
        "    %cd MakeItTalk\n",
        "\n",
        "    img =cv2.imread('examples/' + opt_parser.jpg)\n",
        "    #moved to imports\n",
        "    #predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, device='cpu', flip_input=True)\n",
        "    shapes = predictor.get_landmarks(img)\n",
        "    if (not shapes or len(shapes) != 1):\n",
        "        print('Cannot detect face landmarks. Exit.')\n",
        "        exit(-1)\n",
        "    shape_3d = shapes[0]\n",
        "\n",
        "    if(opt_parser.close_input_face_mouth):\n",
        "        util.close_input_face_mouth(shape_3d)\n",
        "\n",
        "\n",
        "    shape_3d[48:, 0] = (shape_3d[48:, 0] - np.mean(shape_3d[48:, 0])) * LIP_WIDTH_ADJUST + np.mean(shape_3d[48:, 0]) # wider lips\n",
        "    shape_3d[49:54, 1] += UPPER_LIP_ADJUST           # thinner upper lip\n",
        "    shape_3d[55:60, 1] -= LOWER_LIP_ADJUST          # thinner lower lip\n",
        "    shape_3d[[37,38,43,44], 1] -=LARGER_EYE    # larger eyes\n",
        "    shape_3d[[40,41,46,47], 1] +=LARGER_EYE    # larger eyes\n",
        "\n",
        "    shape_3d, scale, shift = util.norm_input_face(shape_3d)\n",
        "\n",
        "\n",
        "\n",
        "def generation(SAY_SOMETHING):\n",
        "    global BACKGROUND\n",
        "    global TEMPERATURE\n",
        "    global MEMORY_LEN\n",
        "\n",
        "    if os.path.exists('/content/MakeItTalk/examples/audio.wav'):\n",
        "        os.remove('/content/MakeItTalk/examples/audio.wav')\n",
        "    if os.path.exists(\"/content/MakeItTalk/examples/my_new_img_pred_fls_audio_audio_embed.mp4\"):\n",
        "        os.remove(\"/content/MakeItTalk/examples/my_new_img_pred_fls_audio_audio_embed.mp4\")\n",
        "    if os.path.exists('/content/MakeItTalk/examples/tmp.wav'):\n",
        "        os.remove('/content/MakeItTalk/examples/tmp.wav')\n",
        "\n",
        "    openai.api_key=OPENAI_API_KEY\n",
        "    text_out=get_response_form_ai(SAY_SOMETHING,\n",
        "                                  openai.api_key,\n",
        "                                  BACKGROUND,\n",
        "                                  TEMPERATURE,\n",
        "                                  MEMORY_LEN)\n",
        "\n",
        "    audio = generate(\n",
        "    text=text_out,\n",
        "    voice=\"Arnold\"\n",
        "    )\n",
        "\n",
        "    with open('/content/MakeItTalk/examples/audio.wav', mode='bx') as f:\n",
        "        f.write(audio)\n",
        "\n",
        "    if os.path.exists('/content/MakeItTalk/examples/M6_04_16k.wav'):\n",
        "        os.remove('/content/MakeItTalk/examples/M6_04_16k.wav')\n",
        "\n",
        "\n",
        "    #%cd MakeItTalk\n",
        "\n",
        "    au_data = []\n",
        "    au_emb = []\n",
        "    ains = glob.glob1('examples', '*.wav')\n",
        "    ains = [item for item in ains if item is not 'tmp.wav']\n",
        "    ains.sort()\n",
        "    for ain in ains:\n",
        "        os.system('ffmpeg -y -loglevel error -i examples/{} -ar 16000 examples/tmp.wav'.format(ain))\n",
        "        shutil.copyfile('examples/tmp.wav', 'examples/{}'.format(ain))\n",
        "\n",
        "        # au embedding\n",
        "        #from thirdparty.resemblyer_util.speaker_emb import get_spk_emb\n",
        "        me, ae = get_spk_emb_prepr('examples/{}'.format(ain), resemblyzer_encoder)\n",
        "        au_emb.append(me.reshape(-1))\n",
        "\n",
        "        print('Processing audio file', ain)\n",
        "        c = AutoVC_mel_Convertor('examples')\n",
        "\n",
        "        au_data_i = c.convert_single_wav_to_autovc_input(audio_filename=os.path.join('examples', ain),\n",
        "              autovc_model_path=opt_parser.load_AUTOVC_name)\n",
        "        au_data += au_data_i\n",
        "    if(os.path.isfile('examples/tmp.wav')):\n",
        "        os.remove('examples/tmp.wav')\n",
        "\n",
        "    # landmark fake placeholder\n",
        "    fl_data = []\n",
        "    rot_tran, rot_quat, anchor_t_shape = [], [], []\n",
        "    for au, info in au_data:\n",
        "        au_length = au.shape[0]\n",
        "        fl = np.zeros(shape=(au_length, 68 * 3))\n",
        "        fl_data.append((fl, info))\n",
        "        rot_tran.append(np.zeros(shape=(au_length, 3, 4)))\n",
        "        rot_quat.append(np.zeros(shape=(au_length, 4)))\n",
        "        anchor_t_shape.append(np.zeros(shape=(au_length, 68 * 3)))\n",
        "\n",
        "    if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl.pickle'))):\n",
        "        os.remove(os.path.join('examples', 'dump', 'random_val_fl.pickle'))\n",
        "    if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))):\n",
        "        os.remove(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))\n",
        "    if(os.path.exists(os.path.join('examples', 'dump', 'random_val_au.pickle'))):\n",
        "        os.remove(os.path.join('examples', 'dump', 'random_val_au.pickle'))\n",
        "    if (os.path.exists(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))):\n",
        "        os.remove(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))\n",
        "\n",
        "    with open(os.path.join('examples', 'dump', 'random_val_fl.pickle'), 'wb') as fp:\n",
        "        pickle.dump(fl_data, fp)\n",
        "    with open(os.path.join('examples', 'dump', 'random_val_au.pickle'), 'wb') as fp:\n",
        "        pickle.dump(au_data, fp)\n",
        "    with open(os.path.join('examples', 'dump', 'random_val_gaze.pickle'), 'wb') as fp:\n",
        "        gaze = {'rot_trans':rot_tran, 'rot_quat':rot_quat, 'anchor_t_shape':anchor_t_shape}\n",
        "        pickle.dump(gaze, fp)\n",
        "\n",
        "\n",
        "    !pwd\n",
        "    model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
        "    if(len(opt_parser.reuse_train_emb_list) == 0):\n",
        "        model.test(au_emb=au_emb)\n",
        "    else:\n",
        "        model.test(au_emb=None)\n",
        "\n",
        "\n",
        "    fls = glob.glob1('examples', 'pred_fls_*.txt')\n",
        "    fls.sort()\n",
        "\n",
        "    for i in range(0,len(fls)):\n",
        "        fl = np.loadtxt(os.path.join('examples', fls[i])).reshape((-1, 68,3))\n",
        "        fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
        "        fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
        "\n",
        "        if (ADD_NAIVE_EYE):\n",
        "            fl = util.add_naive_eye(fl)\n",
        "\n",
        "        # additional smooth\n",
        "        fl = fl.reshape((-1, 204))\n",
        "        fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 15, 3, axis=0)\n",
        "        fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
        "        fl = fl.reshape((-1, 68, 3))\n",
        "\n",
        "        ''' STEP 6: Imag2image translation '''\n",
        "        model = Image_translation_block(opt_parser, single_test=True)\n",
        "        with torch.no_grad():\n",
        "            model.single_test(jpg=img, fls=fl, filename=fls[i], prefix=opt_parser.jpg.split('.')[0])\n",
        "            print('finish image2image gen')\n",
        "        os.remove(os.path.join('examples', fls[i]))\n",
        "\n",
        "        OUTPUT_MP4_NAME = '{}_pred_fls_{}_audio_embed.mp4'.format(\n",
        "                                                                  opt_parser.jpg.split('.')[0],\n",
        "                                                                  ain.split('.')[0]\n",
        "                                                                  )\n",
        "\n",
        "\n",
        "        %cd /content\n",
        "        return \"/content/MakeItTalk/examples/my_new_img_pred_fls_audio_audio_embed.mp4\"#'/content/MakeItTalk/examples/{}'.format(OUTPUT_MP4_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "AgYL4SWalu_8",
        "outputId": "29f35842-64d2-483c-a8b3-329350eec555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:117: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "<>:117: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "<ipython-input-4-50602cd231cb>:117: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  ains = [item for item in ains if item is not 'tmp.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run\n",
        "def setter(lip_x, lip_y, head_pose_motion, naive_eye, close_mouth, upper_lip,\n",
        "           lower_lip, lip_WA, larger_eye, temperature, back, mem_len):\n",
        "\n",
        "    global AMP_LIP_SHAPE_X\n",
        "    AMP_LIP_SHAPE_X=lip_x\n",
        "    global AMP_LIP_SHAPE_Y\n",
        "    AMP_LIP_SHAPE_Y =lip_y\n",
        "    global AMP_HEAD_POSE_MOTION\n",
        "    AMP_HEAD_POSE_MOTION=head_pose_motion\n",
        "    global ADD_NAIVE_EYE\n",
        "    ADD_NAIVE_EYE=naive_eye\n",
        "    global CLOSE_INPUT_FACE_MOUTH\n",
        "    CLOSE_INPUT_FACE_MOUTH=close_mouth\n",
        "    global UPPER_LIP_ADJUST\n",
        "    UPPER_LIP_ADJUST=upper_lip\n",
        "    global LOWER_LIP_ADJUST\n",
        "    LOWER_LIP_ADJUST=lower_lip\n",
        "    global LIP_WIDTH_ADJUST\n",
        "    LIP_WIDTH_ADJUST=lip_WA\n",
        "    global LARGER_EYE\n",
        "    LARGER_EYE=larger_eye\n",
        "    global TEMPERATURE\n",
        "    TEMPERATURE=temperature\n",
        "    global BACKGROUND\n",
        "    BACKGROUND=back\n",
        "    global MEMORY_LEN\n",
        "    MEMORY_LEN =mem_len\n",
        "\n",
        "typer=''\n",
        "def upload_wav(file):\n",
        "    #global voice_path\n",
        "    if os.path.exists('/content/MakeItTalk/examples/my_own_voice_file.wav'):\n",
        "        os.remove('/content/MakeItTalk/examples/my_own_voice_file.wav')\n",
        "    with open('/content/MakeItTalk/examples/my_own_voice_file.wav', mode='bx') as f:\n",
        "        f.write(file)\n",
        "\n",
        "    #file_name=file.name.split('/')[-1]\n",
        "    #shutil.move(file.name, '/content/MakeItTalk/examples/'+file_name)\n",
        "\n",
        "def upload_img(img):\n",
        "    # Load the image as a PIL object\n",
        "    #image = Image.fromarray(img)\n",
        "    global default_head_name\n",
        "    #default_head_name=img.split('/')[-1].split('.')[0]\n",
        "    fn='/content/MakeItTalk/examples/my_new_img.jpg'\n",
        "    if os.path.exists(fn):\n",
        "        os.remove(fn)\n",
        "    #with open(fn, mode='bx') as f:\n",
        "        #f.write(img)\n",
        "    #image=image.save(fn)\n",
        "    shutil.move(img,fn)\n",
        "    default_head_name=fn.split('/')[-1].split('.')[0]\n",
        "\n",
        "    preprocess()\n",
        "\n",
        "\n",
        "def get_video(text):\n",
        "    return generation(text)\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Tab(label = \"Settings\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                lip_x=gr.Slider(0.5, 5, 2, step=0.1, label=\"Lip_shape_X\", interactive=True)\n",
        "                lip_y=gr.Slider(0.5, 5, 2, step=0.1, label=\"Lip_shape_Y\", interactive=True)\n",
        "\n",
        "                naive_eye=gr.Checkbox(value=True, label=\"Add_naive_eye_blink\", interactive=True)\n",
        "                close_inp_mouth=gr.Checkbox(value=False, label=\"Close_input_mouth\", interactive=True)\n",
        "\n",
        "                head_pose_modtion=gr.Slider(0, 1, 0.35, step=0.05, label=\"Head_motion\", interactive=True)\n",
        "                upper_lip=gr.Slider(-1, 3, 1, step=1, label=\"Upper_lip\", interactive=True)\n",
        "                lower_lip=gr.Slider(-1, 3, 1, step=1, label=\"Lower_lip\", interactive=True)\n",
        "                lip_width=gr.Slider(0.8, 1.2, 1, step=0.01, label=\"Lip_width\", interactive=True)\n",
        "                larer_eye=gr.Slider(0, 4, 2, step=0.1, label=\"Larger_eye\", interactive=True)\n",
        "                temperature=gr.Slider(0, 1, 0.7, step=0.01, label=\"Temperature\", interactive=True)\n",
        "                mem_len=gr.Slider(1, 10, 2, step=1, label=\"Memory_len\", interactive=True)\n",
        "\n",
        "            with gr.Column():\n",
        "                im_up=gr.inputs.Image(type=\"filepath\")\n",
        "                #im_up=gr.File(label=\"jpg\", file_types=['image'], type=\"binary\",interactive=True)\n",
        "                im_up.upload(fn=upload_img, inputs=im_up)\n",
        "\n",
        "                file_output = gr.File(label=\"wav\", file_types=['audio'], type=\"binary\",interactive=True)\n",
        "                file_output.upload(upload_wav, inputs=file_output)\n",
        "                #upload_button = gr.UploadButton(\"Upload a Audio\", file_types=[\"audio\"], file_count=\"single\")\n",
        "                #upload_button.upload(upload_wav, upload_button, file_output)\n",
        "\n",
        "                background=gr.Text(lines=7)\n",
        "\n",
        "        btn1 = gr.Button(\"Set\")\n",
        "        btn1.click(fn=setter, inputs=[lip_x, lip_y, head_pose_modtion, naive_eye, close_inp_mouth,\n",
        "                                          upper_lip, lower_lip, lip_width, larer_eye, temperature, background,\n",
        "                                          mem_len])\n",
        "\n",
        "    with gr.Tab(label = \"Generation\"):\n",
        "        show_result=gr.Video(type=\"file\", label=\"result\", autoplay=True, height=512) #interactive=True) #every=20,\n",
        "        response=gr.Text(lines=1)\n",
        "        btn2 = gr.Button(\"Generate\")\n",
        "        btn2.click(fn=get_video, inputs=response, outputs=show_result)\n",
        "\n",
        "demo.launch(debug=True,  enable_queue=True, share=True)#debug=True) #share=True,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "hGgJEKTdi7nu",
        "outputId": "945c659d-9a6f-4120-8865-ca35b12c8a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-91c9167358f4>:81: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  im_up=gr.inputs.Image(type=\"filepath\")\n",
            "<ipython-input-5-91c9167358f4>:81: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  im_up=gr.inputs.Image(type=\"filepath\")\n",
            "<ipython-input-5-91c9167358f4>:98: GradioUnusedKwargWarning: You have unused kwarg parameters in Video, please remove them: {'type': 'file'}\n",
            "  show_result=gr.Video(type=\"file\", label=\"result\", autoplay=True, height=512) #interactive=True) #every=20,\n",
            "<ipython-input-5-91c9167358f4>:103: GradioDeprecationWarning: The `enable_queue` parameter has been deprecated. Please use the `.queue()` method instead.\n",
            "  demo.launch(debug=True,  enable_queue=True, share=True)#debug=True) #share=True,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://824032a0a8e2090f48.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://824032a0a8e2090f48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://824032a0a8e2090f48.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2+2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG76RcNlzTl9",
        "outputId": "d4aa9d06-5836-4fb5-a5e0-ac5e6619f011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}